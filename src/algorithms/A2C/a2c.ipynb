{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.categorical import Categorical\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation, output_activation) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_of_layers = len(layer_sizes)\n",
    "        for i in range(num_of_layers - 1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            activation_function = activation if i < (num_of_layers - 2) else output_activation\n",
    "            layers += [layer, activation_function()]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        action_probs = self.model(x)\n",
    "        return Categorical(probs=action_probs)\n",
    "\n",
    "    def update(self, optimizer, loss):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation, output_activation) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_of_layers = len(layer_sizes)\n",
    "        for i in range(num_of_layers - 1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            activation_function = activation if i < (num_of_layers - 2) else output_activation\n",
    "            layers += [layer, activation_function()]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def update(self, optimizer, loss):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# Agent\n",
    "class ActorCritic: \n",
    "    def __init__(self, env, actor_lr=1e-3, critic_lr=3e-4, gamma=0.99) -> None:\n",
    "        self.env = env\n",
    "        self.actor = Actor([env.observation_space.shape[0], 64, 64, env.action_space.n], nn.ReLU, nn.Softmax)\n",
    "        self.critic = Critic([env.observation_space.shape[0], 64, 64, 1], nn.ReLU, nn.Identity)\n",
    "        self.actor_optimizer = Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def sample_action(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        return self.actor(obs).sample().item()\n",
    "\n",
    "    def compute_actor_loss(self, observations, actions, weights):\n",
    "        observations = torch.stack([torch.tensor(obs, dtype=torch.float32) for obs in observations])\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        logp = self.actor(observations).log_prob(actions)\n",
    "        return -(logp * weights).mean()\n",
    "        \n",
    "    def compute_critic_loss(self, values, rewards):\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        values_tensor = torch.stack(values).squeeze()\n",
    "        return F.mse_loss(rewards, values_tensor)\n",
    "        \n",
    "    def compute_action_values(self, rewards, gamma, values):\n",
    "        action_values = np.zeros_like(rewards)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            action_value = rewards[t] + gamma * values[t]\n",
    "            action_values[t] = action_value\n",
    "        return torch.tensor(action_values, dtype=torch.float32)\n",
    "\n",
    "    def compute_advantage(self, action_values, values):\n",
    "        values_tensor = torch.stack(values)\n",
    "        return action_values - values_tensor.squeeze()\n",
    "\n",
    "    def train(self, epochs=100, episodes=100):\n",
    "        for epoch in range(epochs):\n",
    "            returns, lengths = [], []\n",
    "            for episode in range(episodes):\n",
    "                observations, actions, values = [], [], []\n",
    "                obs, info  = self.env.reset()\n",
    "                terminated = truncated = False\n",
    "                rewards = []\n",
    "                while not terminated and not truncated:\n",
    "                    observations.append(obs)\n",
    "                    action = self.sample_action(obs)\n",
    "                    actions.append(action)\n",
    "                    value = self.critic(torch.tensor(obs.copy(), dtype=torch.float32))\n",
    "                    values.append(value)\n",
    "                    obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "                    rewards.append(reward)\n",
    "                value = self.critic(torch.tensor(obs.copy(), dtype=torch.float32))\n",
    "                action_values = self.compute_action_values(rewards, 0.99, values)\n",
    "                advantages = self.compute_advantage(action_values, values)\n",
    "                actor_loss = self.compute_actor_loss(observations, actions, advantages)\n",
    "                critic_loss = self.compute_critic_loss(values, rewards)\n",
    "                self.actor.update(self.actor_optimizer, actor_loss)\n",
    "                self.critic.update(self.critic_optimizer, critic_loss)\n",
    "                ep_return = sum(rewards)\n",
    "                returns.append(ep_return)\n",
    "                lengths.append(len(rewards))\n",
    "            print(f\"Epoch: {epoch} Return: {mean(returns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahfarr/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Return: 16.89\n",
      "Epoch: 1 Return: 12.75\n",
      "Epoch: 2 Return: 10.52\n",
      "Epoch: 3 Return: 9.91\n",
      "Epoch: 4 Return: 9.56\n",
      "Epoch: 5 Return: 9.69\n",
      "Epoch: 6 Return: 9.63\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m ac \u001b[39m=\u001b[39m ActorCritic(env)\n\u001b[0;32m----> 3\u001b[0m ac\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn[134], line 61\u001b[0m, in \u001b[0;36mActorCritic.train\u001b[0;34m(self, epochs, episodes)\u001b[0m\n\u001b[1;32m     59\u001b[0m critic_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_critic_loss(values, rewards)\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_optimizer, actor_loss)\n\u001b[0;32m---> 61\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic_optimizer, critic_loss)\n\u001b[1;32m     62\u001b[0m ep_return \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(rewards)\n\u001b[1;32m     63\u001b[0m returns\u001b[39m.\u001b[39mappend(ep_return)\n",
      "Cell \u001b[0;32mIn[133], line 17\u001b[0m, in \u001b[0;36mCritic.update\u001b[0;34m(self, optimizer, loss)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, optimizer, loss):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "ac = ActorCritic(env)\n",
    "ac.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit ('3.11.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48dfbf2f4fac079dce67470108b74f2e0481af4046ef9e541d00109928fb3034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
