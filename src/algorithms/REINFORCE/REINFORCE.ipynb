{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm Implementation for CartPole\n",
    "\n",
    "In this project, I will implement the REINFORCE algorithm to train an agent to balance a pole on a cart using the CartPole environment from OpenAI Gym. The objective is to understand the core concepts of the REINFORCE algorithm and gain hands-on experience in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model for representing a policy in reinforcement learning.\n",
    "\n",
    "    Args:\n",
    "        layer_sizes (list[int]): The sizes of each layer in the network.\n",
    "        activation (nn.Module): The activation function used for intermediate layers (default: nn.Tanh).\n",
    "        output_activation (nn.Module): The activation function used for the output layer (default: nn.Identity).\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes: list[int], activation=nn.Tanh, output_activation=nn.Identity):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_of_layers = len(layer_sizes)\n",
    "        for i in range(num_of_layers - 1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            activation_function = activation if i < (num_of_layers - 2) else output_activation\n",
    "            layers += [layer, activation_function()]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the policy network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    A reinforcement learning agent.\n",
    "\n",
    "    Args:\n",
    "        obs_space (int): The size of the observation space.\n",
    "        hidden_sizes (list[int]): The sizes of the hidden layers in the policy network.\n",
    "        n_act (int): The number of actions in the action space.\n",
    "        learning_rate (float): The learning rate for the optimizer (default: 1e-3).\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_space, hidden_sizes, n_act, learning_rate=1e-3):\n",
    "        self.policy_network = PolicyNetwork([obs_space, *hidden_sizes, n_act])\n",
    "        self.optimizer = Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def get_policy(self, obs):\n",
    "        \"\"\"\n",
    "        Get the policy distribution given an observation.\n",
    "\n",
    "        Args:\n",
    "            obs (torch.Tensor): The observation tensor.\n",
    "\n",
    "        Returns:\n",
    "            Categorical: The policy distribution.\n",
    "        \"\"\"\n",
    "        logits = self.policy_network(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        Sample an action from the policy distribution given an observation.\n",
    "\n",
    "        Args:\n",
    "            obs (torch.Tensor): The observation tensor.\n",
    "\n",
    "        Returns:\n",
    "            int: The sampled action.\n",
    "        \"\"\"\n",
    "        return self.get_policy(obs).sample().item()\n",
    "\n",
    "    def compute_loss(self, observations, actions, weights):\n",
    "        \"\"\"\n",
    "        Compute the policy gradient loss.\n",
    "\n",
    "        Args:\n",
    "            obs (torch.Tensor): The observation tensor.\n",
    "            act (torch.Tensor): The action tensor.\n",
    "            weights (torch.Tensor): The weights tensor. In this case, this is the reward of the current episode.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The policy gradient loss.\n",
    "        \"\"\"\n",
    "        logp = self.get_policy(observations).log_prob(actions)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    def store(self, filepath):\n",
    "        \"\"\"\n",
    "        Store the agent's policy network parameters to a file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The file path where the model parameters will be saved.\n",
    "        \"\"\"\n",
    "        torch.save(self.policy_network.state_dict(), filepath)\n",
    "        print(f'Model saved to {filepath}')\n",
    "\n",
    "    def load(self, filepath):\n",
    "        \"\"\"\n",
    "        Load the agent's policy network parameters from a file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The file path where the model parameters are stored.\n",
    "        \"\"\"\n",
    "        self.policy_network.load_state_dict(torch.load(filepath))\n",
    "        self.policy_network.eval()\n",
    "        print(f'Model loaded from {filepath}')\n",
    "\n",
    "    def train(self, env, epochs=100, episodes=500):\n",
    "        \"\"\"\n",
    "        Train the agent in the given environment.\n",
    "\n",
    "        Args:\n",
    "            env (Environment): The environment in which the agent will be trained.\n",
    "            episodes (int): The number of episodes to run the training (default: 500).\n",
    "            render (bool): Whether to render the environment during training (default: False).\n",
    "        \"\"\"\n",
    "        total_returns = np.empty(epochs)\n",
    "        for epoch in range(epochs):\n",
    "            observations, actions, weights = np.empty((0, env.observation_space.shape[0])), [], []\n",
    "            returns, lengths = np.zeros(episodes), np.zeros(episodes, dtype=int)\n",
    "\n",
    "            for episode in range(episodes):\n",
    "                obs, info  = env.reset()\n",
    "                terminated = truncated = False\n",
    "                rewards = []\n",
    "                obs_tensor = torch.tensor(np.array([obs]), dtype=torch.float32)\n",
    "\n",
    "                while not terminated and not truncated:\n",
    "                    observations = np.vstack([observations, obs])\n",
    "                    action = self.sample_action(obs_tensor)\n",
    "                    obs, reward, terminated, truncated, info = env.step(action)\n",
    "                    obs_tensor = torch.tensor(np.array([obs]), dtype=torch.float32)\n",
    "                    actions.append(action)\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                returns[episode] = sum(rewards)\n",
    "                lengths[episode] = len(rewards)\n",
    "                weights.extend([sum(rewards)] * len(rewards))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            actions = np.array(actions, dtype=np.int32)\n",
    "            weights = np.array(weights, dtype=np.float32)\n",
    "            observations = torch.tensor(observations, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.int32)\n",
    "            weights = torch.tensor(weights, dtype=torch.float32)\n",
    "            loss  = self.compute_loss(observations, actions, weights)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_returns[epoch] = mean(returns)\n",
    "            print(f\"Epoch: {epoch}, Return: {mean(returns)}\")\n",
    "        return total_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "obs_space = env.observation_space.shape[0]\n",
    "hidden_sizes = [64, 64, 64, 64]\n",
    "n_acts = env.action_space.n\n",
    "agent = Agent(obs_space=obs_space, hidden_sizes=hidden_sizes, n_act=n_acts)\n",
    "epochs = 50\n",
    "episodes = 100\n",
    "total_returns = agent.train(env, epochs, episodes)\n",
    "agent.store(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Returns\")\n",
    "plt.plot(range(len(total_returns)), total_returns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit ('3.11.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48dfbf2f4fac079dce67470108b74f2e0481af4046ef9e541d00109928fb3034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
