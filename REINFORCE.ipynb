{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm Implementation for CartPole\n",
    "\n",
    "In this project, I will implement the REINFORCE algorithm to train an agent to balance a pole on a cart using the CartPole environment from OpenAI Gym. The objective is to understand the core concepts of the REINFORCE algorithm and gain hands-on experience in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int], activation=nn.Tanh, output_activation=nn.Sigmoid):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_of_layers = len(layer_sizes)\n",
    "        for i in range(num_of_layers - 1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            activation_function = activation if i < num_of_layers - 1 else output_activation\n",
    "            layers += [layer, activation()]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, obs_space, hidden_sizes, n_act, learning_rate=1e-3):\n",
    "        layer_sizes = [obs_space] + hidden_sizes + [n_act]\n",
    "        self.policy_network = Policy(layer_sizes)\n",
    "        self.optimizer = Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def get_policy(self, obs):\n",
    "        logits = self.policy(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        return self.get_policy(obs).sample().item()\n",
    "\n",
    "    def compute_loss(self, obs, act, weights):\n",
    "        logp = self.get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit ('3.11.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48dfbf2f4fac079dce67470108b74f2e0481af4046ef9e541d00109928fb3034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
